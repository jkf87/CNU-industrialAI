{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkf87/CNU-industrialAI/blob/main/%5BRefactoring%5DPrompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnYVzna57lH3"
      },
      "source": [
        "# 프롬프트 엔지니어링 시작하기\n",
        "by DAIR.AI | 엘비스 사라비아\n",
        "by 번역, 리팩토링 [코난쌤](https://www.youtube.com/@conanssam)\n",
        "\n",
        "\n",
        "이 노트북에는 프롬프트 엔지니어링에 대해 배울 수 있는 예제와 연습 문제가 포함되어 있습니다.\n",
        "\n",
        "모든 예제에는 [OpenAI API](https://platform.openai.com/)를 사용합니다. 기본 설정인 'temperature=0.7'과 'top-p=1'을 사용하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvGQ58407lH5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7UHXFmg7lH5"
      },
      "source": [
        "## 1. Prompt Engineering Basics\n",
        "\n",
        "\n",
        "목표\n",
        "- 라이브러리 로드\n",
        "- 포멧 리뷰하기\n",
        "- 기본 프롬프트 다루기\n",
        "- 일반적인 사용 사례 검토"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F3N4pbv7lH5"
      },
      "source": [
        "아래에서 필요한 라이브러리, 유틸리티 및 구성을 로드하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PIDr76g57lH5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 레거시로 돌리려면 지우고 0.28.1 버전을 설치하기\n",
        "!pip uninstall openai\n",
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "id": "HzG3sFI5_iTa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7ROp-lQA7lH6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb_TtHl37lH6"
      },
      "source": [
        "코랩에 보안 비밀 키를 활용하세요.\n",
        "\n",
        "![보안 비밀 키 설정하기](https://i.imgur.com/OCPB4eh.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i8f0l6wu7lH7"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# API configuration\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# for LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기존 레거시 코드 전체 리펙토링\n",
        "\n",
        "- 파라미터 값 설정\n",
        "- 파라미터 값 전달 방법 변경\n",
        "  - 딕셔너리 방법에서 함수 매개변수 방법으로\n",
        "  -\n",
        "  레거시\n",
        "\n",
        "  ```\n",
        "  def set_open_params(\n",
        "    model=\"text-davinci-003\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "\n",
        "    openai_params = {}    \n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "def get_completion(params, prompt):\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine = params['model'],\n",
        "        prompt = prompt,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response\n",
        "  ```\n",
        "\n",
        "  - 리펙토링\n",
        "```\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=20,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    return {\n",
        "        'model': model,\n",
        "        'temperature': temperature,\n",
        "        'max_tokens': max_tokens,\n",
        "        'top_p': top_p,\n",
        "        'frequency_penalty': frequency_penalty,\n",
        "        'presence_penalty': presence_penalty\n",
        "    }\n",
        "def get_completion_v1(params, prompt):\n",
        "    response =  client.completions.create(\n",
        "        **params,  # 딕셔너리의 키-값 쌍을 매개변수로 전달\n",
        "        prompt=prompt\n",
        "    )\n",
        "    return response.choices[0].text\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "BM5r-EDZHPpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=20,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    return {\n",
        "        'model': model,\n",
        "        'temperature': temperature,\n",
        "        'max_tokens': max_tokens,\n",
        "        'top_p': top_p,\n",
        "        'frequency_penalty': frequency_penalty,\n",
        "        'presence_penalty': presence_penalty\n",
        "    }\n",
        "def get_completion(params, prompt):\n",
        "    response =  client.completions.create(\n",
        "        **params,  # 딕셔너리의 키-값 쌍을 매개변수로 전달\n",
        "        prompt=prompt\n",
        "    )\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "MXIzdthiD-i1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_open_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWWySVjhFhPX",
        "outputId": "055d59dc-3531-478b-a592-1ecf5c02e8b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'gpt-3.5-turbo-instruct',\n",
              " 'temperature': 0.7,\n",
              " 'max_tokens': 20,\n",
              " 'top_p': 1,\n",
              " 'frequency_penalty': 0,\n",
              " 'presence_penalty': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_35tJNg7lH7"
      },
      "source": [
        "Basic prompt example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPfpiYsR7lH7",
        "outputId": "a6d982c5-c81e-441f-8942-74d36429343a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "네, 코난쌤은 제가 아는 분입니다.\n"
          ]
        }
      ],
      "source": [
        "# basic example\n",
        "params = set_open_params()\n",
        "\n",
        "prompt = \"안녕 코난쌤을 아세요?\"\n",
        "\n",
        "gpt_response=get_completion(params, prompt)\n",
        "\n",
        "print(gpt_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "UdF51mWd7lH8",
        "outputId": "f3df2878-fb48-4c85-e9a4-f21773184027"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n네, 코난쌤은 제가 아는 분입니다."
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "IPython.display.Markdown(gpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RLhNF07lH8"
      },
      "source": [
        "다른 temperature로 시도하여 결과를 비교하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "RQxgl8Iq7lH8",
        "outputId": "d288ca41-2930-47b3-f940-bf376fe693af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n네, 코난쌤은 일본의 만화가"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "params = set_open_params(temperature=0)\n",
        "prompt = \"안녕 코난쌤을 아세요?\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge9Mhzr97lH9"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vTzwt6p47lH9",
        "outputId": "0317de2a-ac09-43e7-9b44-9f3e08eed1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Antibiotics are medicines that fight bacterial infections by killing the bacteria or preventing their reproduction, and can be"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuXj6Vi7lH9"
      },
      "source": [
        "Exercise: 모델에게 \"나는 5살이다\"와 같이 한 문장으로 단락을 설명하도록 지시하기. 차이가 보이나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Jgxy3L7lH9"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y8zvw8ny7lH9",
        "outputId": "706d4dc8-b7cd-4050-9b38-53e0b6ffc959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Mice"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXPCEHtO7lH9"
      },
      "source": [
        "여기에서 얻은 컨텍스트: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSnqVnuP7lH9"
      },
      "source": [
        "Exercise:수정 프롬프트에서 모델이 답을 잘 모른다고 응답하도록 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9wX3bb7lH9"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aPE_6gZf7lH-",
        "outputId": "47e235bf-dac6-48d7-c8d8-d068f41754bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Neutral"
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XSgBt5t7lH-"
      },
      "source": [
        "Exercise: 모델에게 선택한 답에 대한 설명을 제공하도록 프롬프트를 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYIpwiqf7lH-"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MahL85bC7lH-",
        "outputId": "dfea23b6-52a4-4125-bd23-21971a65f39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Of course. Black holes are formed when a massive star dies and its core collapses under its own gravity"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4p0PFCB7lH-"
      },
      "source": [
        "Exercise: AI의 반응을 간결하고 짧게 유지하도록 지시하는 프롬프트 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-SnIacp7lH-"
      },
      "source": [
        "### 1.5 Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "InA7_NLQ7lH-",
        "outputId": "3168c94b-eee1-4424-fc87-c26f6e9e32be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSELECT StudentName \nFROM students \nWHERE DepartmentId = (SELECT DepartmentId \n                      FROM departments"
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFlfnXJk7lH-"
      },
      "source": [
        "### 1.6 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V0ERCAMJ7lH_",
        "outputId": "076ffd42-dab8-4972-f76a-7e15eaddfa80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nStep 1: Identify the odd numbers in the group.\nIn this group, the odd numbers"
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsJxddcH7lH_"
      },
      "source": [
        "Exercise: 프롬프트가 더 나은 구조와 출력 형식을 갖도록 개선해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_ePAE17lH_"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "목표:\n",
        "\n",
        "- 프롬프트에 대한 고급 기법(few-shot, chain-of-thoughts,...)을 다룹니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT-Kgp6O7lH_"
      },
      "source": [
        "### 2.2 Few-shot prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zO8vI6D07lH_",
        "outputId": "3aaf713c-9931-449a-e398-b958b25d0a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The answer is False."
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLQvDjJ7lH_"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUMQUomA7lIA",
        "outputId": "35ef6529-1312-40eb-f306-2f6767ea8cc2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybNDXsRd7lIA"
      },
      "source": [
        "### 2.4 Zero-shot CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DnnoNsUd7lIA",
        "outputId": "8ae46374-cab2-41e4-bd4a-1826026c353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " \n1. Bought 10 apples\n2. Gave 2 apples to neighbor and 2"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nrffd-U7lIB"
      },
      "source": [
        "### 2.5 자체 일관성 유지\n",
        "연습 삼아 [가이드](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#self-consistency)에서 예시를 확인하고 여기에서 시도해 보세요.\n",
        "\n",
        "### 2.6 지식 프롬프트 생성하기\n",
        "\n",
        "연습용으로 [가이드](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#generated-knowledge-prompting)에서 예시를 확인하고 여기에서 시도해 보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcgUnlov7lIB"
      },
      "source": [
        "### 2.6 PAL - Code as Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FggYqKU7lIB"
      },
      "source": [
        "코드를 통해서 질문에 대해 추론을 할 수 있는 간단한 애플리케이션을 개발해보겠습니다.\n",
        "\n",
        "구체적으로, 이 애플리케이션은 일부 데이터를 가져와서 입력된 데이터에 대한 질문에 답합니다. 프롬프트에는 [여기](https://github.com/reasoning-machines/pal/blob/main/pal/prompt/penguin_prompt.py)에서 채택한 몇 가지 예제가 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hVWeWQT67lIB"
      },
      "outputs": [],
      "source": [
        "# 기존 레거시 코드\n",
        "# lm instance\n",
        "# llm = OpenAI(model='text-davinci-003', temperature=0)\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model='text-davinci-003', temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "avPve9YL7lIB"
      },
      "outputs": [],
      "source": [
        "question = \"누가 더 늙은 팽귄인가요?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "o7SpLmZ97lIB"
      },
      "outputs": [],
      "source": [
        "PENGUIN_PROMPT = '''\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "We now add a penguin to the table:\n",
        "James, 12, 90, 12\n",
        "How many penguins are less than 8 years old?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Add penguin James.\n",
        "penguins.append(('James', 12, 90, 12))\n",
        "# Find penguins under 8 years old.\n",
        "penguins_under_8_years_old = [penguin for penguin in penguins if penguin[1] < 8]\n",
        "# Count number of penguins under 8.\n",
        "num_penguin_under_8 = len(penguins_under_8_years_old)\n",
        "answer = num_penguin_under_8\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "Which is the youngest penguin?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort the penguins by age.\n",
        "penguins = sorted(penguins, key=lambda x: x[1])\n",
        "# Get the youngest penguin's name.\n",
        "youngest_penguin_name = penguins[0][0]\n",
        "answer = youngest_penguin_name\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "What is the name of the second penguin sorted by alphabetic order?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort penguins by alphabetic order.\n",
        "penguins_alphabetic = sorted(penguins, key=lambda x: x[0])\n",
        "# Get the second penguin sorted by alphabetic order.\n",
        "second_penguin_name = penguins_alphabetic[1][0]\n",
        "answer = second_penguin_name\n",
        "\"\"\"\n",
        "{question}\n",
        "\"\"\"\n",
        "'''.strip() + '\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZXsrjrI7lIB"
      },
      "source": [
        "이제 프롬프트와 질문이 생겼습니다. 이제 이를 모델에 전송할 수 있습니다. 답에 대한 해답을 얻는 데 필요한 단계를 코드로 출력해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "pU6qitIX7lIB",
        "outputId": "e3d2ea92-87a9-47d2-b438-14bf95553917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Put the penguins into a list.\n",
            "penguins = []\n",
            "penguins.append(('Louis', 7, 50, 11))\n",
            "penguins.append(('Bernard', 5, 80, 13))\n",
            "penguins.append(('Vincent', 9, 60, 11))\n",
            "penguins.append(('Gwen', 8, 70, 15))\n",
            "# Sort penguins by age.\n",
            "penguins_by_age = sorted(penguins, key=lambda x: x[1], reverse=True)\n",
            "# Get the oldest penguin's name.\n",
            "oldest_penguin_name = penguins_by_age[0][0]\n",
            "answer = oldest_penguin_name\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = PENGUIN_PROMPT.format(question=question)\n",
        "llm_out = llm(formatted_prompt)\n",
        "\n",
        "print(llm_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "rlh5gedn7lIC",
        "outputId": "76cc6668-b4d8-4fc1-c988-b0eaca93a644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vincent\n"
          ]
        }
      ],
      "source": [
        "exec(llm_out)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tllKsevU7lIC"
      },
      "source": [
        "정답입니다! 빈센트는 가장 나이가 많은 펭귄입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAeLRfqP7lIC"
      },
      "source": [
        "Exercise: 다른 질문을 해보고 어떤 결과가 나오는지 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 다른 질문 예시\n",
        "\n",
        "# question = \"누가 제일 키가 큰 팽귄인가요?\"\n",
        "# question = \"누가 가장 몸무게가 많이 나가는 팽귄인가요?\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "PAPjHf7q6Dix"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKsA4dLa7lIC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWIalBKU7lIC"
      },
      "source": [
        "# 3. 도구 및 애플리케이션\n",
        "\n",
        "목표:\n",
        "\n",
        "- 프롬프트 기술과 LLM을 사용하여 간단한 애플리케이션을 시연하기 위해 LangChain을 사용하는 방법을 시연합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW9IDAB97lIC"
      },
      "source": [
        "### 3.1 LLM 및 외부 도구\n",
        "\n",
        "[LangChain 문서](https://langchain.readthedocs.io/en/latest/modules/agents/getting_started.html)에서 채택한 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ekH11z2i7lIC"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "pAETNC7c7lID"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ZEaB_8F57lID",
        "outputId": "d403082a-d0ec-4bed-a392-14c7a14a6f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
            "Action: google_serper\n",
            "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mWilde did not seek spousal support, and the pair reached a private agreement on property division. Wilde began dating American actor and comedian Jason Sudeikis in November 2011. They became engaged in January 2013. The couple have two children: a son, born in 2014, and a daughter, born in 2016.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out Jason Sudeikis' age.\n",
            "Action: google_serper\n",
            "Action Input: Jason Sudeikis age\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m48 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate his age raised to the 0.23 power.\n",
            "Action: Calculator\n",
            "Action Input: 48^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.436045998596883\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Jason Sudeikis, Olivia Wilde's boyfriend, is 48 years old and his age raised to the 0.23 power is 2.436045998596883.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Jason Sudeikis, Olivia Wilde's boyfriend, is 48 years old and his age raised to the 0.23 power is 2.436045998596883.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# run the agent\n",
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbEnlGH7lID"
      },
      "source": [
        "### 3.2 Data-Augmented Generation (문서 기반 응답)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82tADUE7lID"
      },
      "source": [
        "먼저, 증강 생성을 위한 소스로 사용할 데이터를 다운로드해야 합니다.\n",
        "\n",
        "코드 예제는 [LangChain 문서](https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/qa_with_sources.html)에서 가져온 것입니다. 예제는 교육 목적으로만 사용하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYWWgJJK7lID"
      },
      "source": [
        "먼저 데이터를 준비합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5oW5rdm_7lID"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "oa9J9xh97lID"
      },
      "outputs": [],
      "source": [
        "with open('./state_of_the_union.txt') as f:\n",
        "    state_of_the_union = f.read()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HRn0csA7yEe",
        "outputId": "2e5a0b65-dacf-442e-eb66-4fd94e56a624"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.17)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.104.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.24.0.post1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (28.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.42b0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kuw6Xwlm7lIE"
      },
      "outputs": [],
      "source": [
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nDK7cZYN7lIE"
      },
      "outputs": [],
      "source": [
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLkXjpsj7lIE"
      },
      "source": [
        "빠르게 테스트해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "SgNJ-Jl-7lIE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Po5WsxSi7lIE",
        "outputId": "484757b4-e173-486c-a8a2-ef6d75de61c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': \" The president thanked Justice Breyer for his service and mentioned that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to continue Justice Breyer's legacy of excellence.\\nSOURCES: 31-pl\"}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhzZe747lIE"
      },
      "source": [
        "사용자 지정 프롬프트가 있는 질문을 해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gA8V7EEN7lIE",
        "outputId": "35dba988-3e27-4579-928b-8c4d1d921a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': '\\nEl Presidente dijo que Justice Breyer ha dedicado su vida a servir a este país y que el nombramiento de la Juez Ketanji Brown Jackson como su sucesora continuará el legado de excelencia de Breyer.\\n\\nFUENTES:\\n31. Discurso del Presidente Biden ante el Congreso, 25 de abril de 2021. Disponible en: https://www.whitehouse.gov/briefing-room/speeches-remarks/2021/04/28/remarks-by-president-biden-address-to-a-joint-session-of-congress/'}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "ALWAYS return a \"SOURCES\" part in your answer.\n",
        "Respond in Spanish.\n",
        "\n",
        "QUESTION: {question}\n",
        "=========\n",
        "{summaries}\n",
        "=========\n",
        "FINAL ANSWER IN SPANISH:\"\"\"\n",
        "\n",
        "# create a prompt template\n",
        "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
        "\n",
        "# query\n",
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
        "query = \"What did the president say about Justice Breyer?\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gSJ6khR7lIF"
      },
      "source": [
        "Exercise: 지금까지 예제에서 배운 모든 기법을 사용하고 인터넷에 있는 다른 데이터셋과 다른 프롬프트를 사용해보기"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
